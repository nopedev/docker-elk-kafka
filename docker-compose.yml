version: "3.7"

services:
  # The 'setup' service runs a one-off script which initializes the
  # 'logstash_internal' and 'kibana_system' users inside Elasticsearch with the
  # values of the passwords defined in the '.env' file.
  #
  # This task is only performed during the *initial* startup of the stack. On all
  # subsequent runs, the service simply returns immediately, without performing
  # any modification to existing users.
  setup:
    build:
      context: setup/
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    init: true
    volumes:
      - setup:/state:Z
    environment:
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD:-}
      LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-}
      KIBANA_SYSTEM_PASSWORD: ${KIBANA_SYSTEM_PASSWORD:-}
    networks:
      - elk

  elasticsearch:
    build:
      context: elasticsearch/
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    volumes:
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro,z
      - elasticsearch:/usr/share/elasticsearch/data:z
      - ./sample-data:/usr/share/elasticsearch/sameple-data
      - ./elasticsearch/config/user_dic:/usr/share/elasticsearch/config/user_dic
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: -Xmx512m -Xms512m
      # Bootstrap password.
      # Used to initialize the keystore during the initial startup of
      # Elasticsearch. Ignored on subsequent runs.
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD:-}
      # Use single node discovery in order to disable production mode and avoid bootstrap checks.
      # see: https://www.elastic.co/guide/en/elasticsearch/reference/7.17/bootstrap-checks.html
      discovery.type: single-node
    networks:
      - elk

  logstash:
    build:
      context: logstash/
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro,Z
      - ./logstash/config/pipelines.yml:/usr/share/logstash/config/pipelines.yml:ro,Z
      - ./logstash/pipeline:/usr/share/logstash/pipeline:Z
      # - ./logstash/pipeline:/usr/share/logstash/pipeline:ro,Z
    ports:
      - "5044:5044"
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: -Xmx256m -Xms256m
      LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-}
    networks:
      - elk
    depends_on:
      - elasticsearch
      # - kafka

  kibana:
    build:
      context: kibana/
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    volumes:
      - ./kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro,Z
    ports:
      - "5601:5601"
    environment:
      KIBANA_SYSTEM_PASSWORD: ${KIBANA_SYSTEM_PASSWORD:-}
    networks:
      - elk
    depends_on:
      - elasticsearch

  # zookeeper:
  #   image: wurstmeister/zookeeper
  #   ports:
  #     - "2181:2181"
  #   networks:
  #     - elk

  # kafka:
  #   build: kafka/
  #   ports:
  #     - "29092:29092"
  #   environment:
  #     # KAFKA_ADVERTISED_HOST_NAME: kafka
  #     # HOSTNAME_COMMAND: "route -n | awk '/UG[ \t]/{print $$2}'"
  #     KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://127.0.0.1:29092
  #     KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:29092
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
  #     KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
  #     # topic1:1:1 => topic1 -> 1, poaritions, 1 replications
  #     KAFKA_CREATE_TOPICS: "otv-test:1:1,otv-api-logs:1:1,otv-ott-logs:1:1"
  #     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
  #     KAFKA_AUTO_CREATE_TOPICS_ENABLE: false
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  #     KAFKA_NUM_PARTITIONS: 1
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock
  #     - ./kafka/logs:/opt/kafka/logs
  #   depends_on:
  #     - zookeeper
  #   networks:
  #     - elk

networks:
  elk:
    driver: bridge

volumes:
  setup:
  elasticsearch:
